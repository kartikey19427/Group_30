{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4ec1c87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4ec1c87",
    "outputId": "2e6c8968-8605-4037-dd11-eb4c0dfa8f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (23.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e59567f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e59567f",
    "outputId": "68c11242-6074-40bb-c3c8-4fbe7e5da67f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.8.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from webdriver-manager) (4.64.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from webdriver-manager) (1.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from webdriver-manager) (2.27.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->webdriver-manager) (3.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->webdriver-manager) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->webdriver-manager) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->webdriver-manager) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->webdriver-manager) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9dacd377",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dacd377",
    "outputId": "73e79570-a7d1-4aac-d084-e3a7dd9ab6c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium==4.2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from selenium==4.2.0) (0.9.2)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from selenium==4.2.0) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from selenium==4.2.0) (0.22.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium==4.2.0) (2.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium==4.2.0) (1.1.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium==4.2.0) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium==4.2.0) (21.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium==4.2.0) (3.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium==4.2.0) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium==4.2.0) (1.15.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium==4.2.0) (1.10)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio-websocket~=0.9->selenium==4.2.0) (1.2.0)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium==4.2.0) (23.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium==4.2.0) (39.0.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium==4.2.0) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from urllib3[secure,socks]~=1.26->selenium==4.2.0) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium==4.2.0) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium==4.2.0) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium==4.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93587e6c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93587e6c",
    "outputId": "1af99484-d438-4481-fce0-56a24a842139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: linkedin_scraper in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (2.11.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: selenium in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from linkedin_scraper) (4.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from linkedin_scraper) (2.27.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from linkedin_scraper) (4.9.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->linkedin_scraper) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->linkedin_scraper) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->linkedin_scraper) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->linkedin_scraper) (3.3)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from selenium->linkedin_scraper) (0.9.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from selenium->linkedin_scraper) (0.22.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.1.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.15.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium->linkedin_scraper) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from trio-websocket~=0.9->selenium->linkedin_scraper) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from urllib3<1.27,>=1.21.1->requests->linkedin_scraper) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urllib3<1.27,>=1.21.1->requests->linkedin_scraper) (23.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from urllib3<1.27,>=1.21.1->requests->linkedin_scraper) (39.0.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\pc\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->linkedin_scraper) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->linkedin_scraper) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "pip install --user linkedin_scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28f292a5",
   "metadata": {
    "id": "28f292a5"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from linkedin_scraper import Person, actions\n",
    "import requests\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from linkedin_scraper.objects import Experience, Education, Scraper, Interest, Accomplishment, Contact\n",
    "import os\n",
    "from linkedin_scraper import selectors\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "__TOP_CARD = \"pv-top-card\"\n",
    "__WAIT_FOR_ELEMENT_TIMEOUT = 5\n",
    "\n",
    "\n",
    "def scrape(scraper, close_on_complete=True):\n",
    "        if scraper.is_signed_in():\n",
    "            scrape_logged_in(scraper, close_on_complete=close_on_complete)\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "def scrape_logged_in(self, close_on_complete=True):\n",
    "        driver = self.driver\n",
    "        duration = None\n",
    "\n",
    "        root = WebDriverWait(driver, __WAIT_FOR_ELEMENT_TIMEOUT).until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.CLASS_NAME,\n",
    "                    __TOP_CARD,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.focus()\n",
    "        self.wait(5)\n",
    "\n",
    "        # get name and location\n",
    "        self.get_name_and_location()\n",
    "\n",
    "        self.open_to_work = self.is_open_to_work()\n",
    "\n",
    "        # get about\n",
    "        get_about(self)\n",
    "        driver.execute_script(\n",
    "            \"window.scrollTo(0, Math.ceil(document.body.scrollHeight/2));\"\n",
    "        )\n",
    "        driver.execute_script(\n",
    "            \"window.scrollTo(0, Math.ceil(document.body.scrollHeight/1.5));\"\n",
    "        )\n",
    "\n",
    "        # get experience\n",
    "        try:\n",
    "            get_experiences(self)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # get education\n",
    "        try:\n",
    "            get_educations(self)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        driver.get(self.linkedin_url)\n",
    "        \n",
    "        # get skills\n",
    "        try:\n",
    "            get_skills(self)\n",
    "        except:\n",
    "            self.skills = []\n",
    "            pass\n",
    "        \n",
    "        # get courses\n",
    "        try:\n",
    "            get_courses(self)\n",
    "        except:\n",
    "            self.courses = []\n",
    "            pass\n",
    "\n",
    "        if close_on_complete:\n",
    "            driver.quit()\n",
    "            \n",
    "def get_about(self):\n",
    "        try:\n",
    "            about = self.driver.find_element_by_id(\"about\").find_element_by_xpath(\"..\").find_element_by_class_name(\"display-flex\").text\n",
    "            self.about = about\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        \n",
    "def get_experiences(self):\n",
    "        url = os.path.join(self.linkedin_url, \"details/experience\")\n",
    "        self.driver.get(url)\n",
    "        self.focus()\n",
    "        main = self.wait_for_element_to_load(by=By.ID, name=\"main\")\n",
    "        self.scroll_to_half()\n",
    "        self.scroll_to_bottom()\n",
    "        main_list = self.wait_for_element_to_load(name=\"pvs-list\", base=main)\n",
    "        for position in main_list.find_elements_by_xpath(\"li\"):\n",
    "            position = position.find_element_by_class_name(\"pvs-entity\")\n",
    "            company_logo_elem, position_details = position.find_elements_by_xpath(\"*\")\n",
    "            # company elem\n",
    "            company_linkedin_url = company_logo_elem.find_element_by_xpath(\"*\").get_attribute(\"href\")\n",
    "\n",
    "            # position details\n",
    "            position_details_list = position_details.find_elements_by_xpath(\"*\")\n",
    "            position_summary_details = position_details_list[0] if len(position_details_list) > 0 else None\n",
    "            position_summary_text = position_details_list[1] if len(position_details_list) > 1 else None\n",
    "            outer_positions = position_summary_details.find_element_by_xpath(\"*\").find_elements_by_xpath(\"*\")\n",
    "        \n",
    "            if len(outer_positions) == 4:\n",
    "                position_title = outer_positions[0].find_element_by_tag_name(\"span\").find_element_by_tag_name(\"span\").text\n",
    "                company = outer_positions[1].find_element_by_tag_name(\"span\").text\n",
    "                work_times = outer_positions[2].find_element_by_tag_name(\"span\").text\n",
    "                location = outer_positions[3].find_element_by_tag_name(\"span\").text\n",
    "            elif len(outer_positions) == 3:\n",
    "                if \"·\" in outer_positions[2].text:\n",
    "                    position_title = outer_positions[0].find_element_by_tag_name(\"span\").find_element_by_tag_name(\"span\").text\n",
    "                    company = outer_positions[1].find_element_by_tag_name(\"span\").text\n",
    "                    work_times = outer_positions[2].find_element_by_tag_name(\"span\").text\n",
    "                    location = \"\"\n",
    "                else:\n",
    "                    position_title = \"\"\n",
    "                    company = outer_positions[0].find_element_by_tag_name(\"span\").find_element_by_tag_name(\"span\").text\n",
    "                    work_times = outer_positions[1].find_element_by_tag_name(\"span\").text\n",
    "                    location = outer_positions[2].find_element_by_tag_name(\"span\").text\n",
    "            elif len(outer_positions) == 2:\n",
    "                        position_title = \"\"\n",
    "                        company = outer_positions[0].find_element_by_tag_name(\"span\").find_element_by_tag_name(\"span\").text\n",
    "                        work_times = outer_positions[1].find_element_by_tag_name(\"span\").text\n",
    "                        location = \"\"\n",
    "                    \n",
    "                    \n",
    "            times = work_times.split(\"·\")[0].strip() if work_times else \"\"\n",
    "            duration = work_times.split(\"·\")[1].strip() if len(work_times.split(\"·\")) > 1 else None\n",
    "\n",
    "            from_date = \" \".join(times.split(\" \")[:2]) if times else \"\"\n",
    "            to_date = \" \".join(times.split(\" \")[3:]) if times else \"\"\n",
    "            \n",
    "\n",
    "            if position_summary_text and len(position_summary_text.find_element_by_class_name(\"pvs-list\").find_element_by_class_name(\"pvs-list\").find_elements_by_xpath(\"li\")) > 1:\n",
    "                descriptions = position_summary_text.find_element_by_class_name(\"pvs-list\").find_element_by_class_name(\"pvs-list\").find_elements_by_xpath(\"li\")\n",
    "                for description in descriptions:\n",
    "                    res = description.find_element_by_tag_name(\"a\").find_elements_by_xpath(\"*\")\n",
    "                    position_title_elem = res[0] if len(res) > 0 else None\n",
    "                    \n",
    "                    if len(res) == 2:\n",
    "                        work_times_elem = res[1] \n",
    "                        location_elem = None\n",
    "                    elif len(res) == 3:\n",
    "                        if \"·\" in res[2].find_element_by_xpath(\"*\").text:\n",
    "                            work_times_elem = res[2]\n",
    "                            location_elem = None\n",
    "                        else:\n",
    "                            work_times_elem = res[1] \n",
    "                            location_elem = res[2]\n",
    "                    elif len(res) == 4:\n",
    "                        work_times_elem = res[2] \n",
    "                        location_elem = res[3]\n",
    "                    else:\n",
    "                        work_times_elem = None\n",
    "                        location_elem = None\n",
    "\n",
    "                    location = location_elem.find_element_by_xpath(\"*\").text if location_elem else None\n",
    "                    position_title = position_title_elem.find_element_by_xpath(\"*\").find_element_by_tag_name(\"*\").text if position_title_elem else \"\"\n",
    "                    work_times = work_times_elem.find_element_by_xpath(\"*\").text if work_times_elem else \"\"\n",
    "                    times = work_times.split(\"·\")[0].strip() if work_times else \"\"\n",
    "                    duration = work_times.split(\"·\")[1].strip() if len(work_times.split(\"·\")) > 1 else None\n",
    "                    from_date = \" \".join(times.split(\" \")[:2]) if times else \"\"\n",
    "                    to_date = \" \".join(times.split(\" \")[3:]) if times else \"\"\n",
    "\n",
    "                    experience = Experience(\n",
    "                        position_title=position_title,\n",
    "                        from_date=from_date,\n",
    "                        to_date=to_date,\n",
    "                        duration=duration,\n",
    "                        location=location,\n",
    "                        description=\"\",\n",
    "                        institution_name=company,\n",
    "                        linkedin_url=company_linkedin_url\n",
    "                    )\n",
    "                    self.add_experience(experience)\n",
    "            else:\n",
    "                description = position_summary_text.find_element_by_tag_name(\"span\").text if position_summary_text else \"\"\n",
    "\n",
    "                experience = Experience(\n",
    "                    position_title=position_title,\n",
    "                    from_date=from_date,\n",
    "                    to_date=to_date,\n",
    "                    duration=duration,\n",
    "                    location=location,\n",
    "                    description=description,\n",
    "                    institution_name=company,\n",
    "                    linkedin_url=company_linkedin_url\n",
    "                )\n",
    "                self.add_experience(experience)\n",
    "\n",
    "def get_educations(self):\n",
    "        url = os.path.join(self.linkedin_url, \"details/education\")\n",
    "        self.driver.get(url)\n",
    "        self.focus()\n",
    "        main = self.wait_for_element_to_load(by=By.ID, name=\"main\")\n",
    "        self.scroll_to_half()\n",
    "        self.scroll_to_bottom()\n",
    "        main_list = self.wait_for_element_to_load(name=\"pvs-list\", base=main)\n",
    "        for position in main_list.find_elements_by_class_name(\"pvs-entity\"):\n",
    "            institution_logo_elem, position_details = position.find_elements_by_xpath(\"*\")\n",
    "\n",
    "            # company elem\n",
    "            institution_linkedin_url = institution_logo_elem.find_element_by_xpath(\"*\").get_attribute(\"href\")\n",
    "\n",
    "            # position details\n",
    "            position_details_list = position_details.find_elements_by_xpath(\"*\")\n",
    "            position_summary_details = position_details_list[0] if len(position_details_list) > 0 else None\n",
    "            position_summary_text = position_details_list[1] if len(position_details_list) > 1 else None\n",
    "            outer_positions = position_summary_details.find_element_by_xpath(\"*\").find_elements_by_xpath(\"*\")\n",
    "            \n",
    "            if len(outer_positions) == 1:\n",
    "                institution_name = outer_positions[0].find_element_by_tag_name(\"span\").find_element_by_tag_name(\"span\").text\n",
    "                degree = None\n",
    "                times = None\n",
    "            elif len(outer_positions) == 2:\n",
    "                institution_name = outer_positions[0].find_element_by_tag_name(\"span\").find_element_by_tag_name(\"span\").text\n",
    "                if \" - \" in outer_positions[1].find_element_by_tag_name(\"span\").text and len(outer_positions[1].find_element_by_tag_name(\"span\").text)==5 and \" \".join(times.split(\" \")[:1]).isnumeric() and \" \".join(times.split(\" \")[2:]).isnumeric():\n",
    "                    degree = None\n",
    "                    times = outer_positions[1].find_element_by_tag_name(\"span\").text\n",
    "                else:\n",
    "                    degree = outer_positions[1].find_element_by_tag_name(\"span\").text\n",
    "                    times = None\n",
    "            elif len(outer_positions) == 3:\n",
    "                institution_name = outer_positions[0].find_element_by_tag_name(\"span\").find_element_by_tag_name(\"span\").text\n",
    "                degree = outer_positions[1].find_element_by_tag_name(\"span\").text\n",
    "                times = outer_positions[2].find_element_by_tag_name(\"span\").text\n",
    "\n",
    "            if times is not None:\n",
    "                from_date = \" \".join(times.split(\" \")[:1])\n",
    "                to_date = \" \".join(times.split(\" \")[2:])\n",
    "            else:\n",
    "                from_date = \" \"\n",
    "                to_date = \" \"\n",
    "\n",
    "            description = position_summary_text.find_element_by_tag_name(\"span\").text if position_summary_text else \"\"\n",
    "\n",
    "            education = Education(\n",
    "                from_date=from_date,\n",
    "                to_date=to_date,\n",
    "                description=description,\n",
    "                degree=degree,\n",
    "                institution_name=institution_name,\n",
    "                linkedin_url=institution_linkedin_url\n",
    "            )\n",
    "            self.add_education(education)\n",
    "            \n",
    "\n",
    "def get_skills(self):\n",
    "        url = os.path.join(self.linkedin_url, \"details/skills\")\n",
    "        self.driver.get(url)\n",
    "        self.focus()\n",
    "        main = self.wait_for_element_to_load(by=By.ID, name=\"main\")\n",
    "        self.scroll_to_half()\n",
    "        self.scroll_to_bottom()\n",
    "        main_list = self.wait_for_element_to_load(name=\"pvs-list\", base=main)\n",
    "        self.skills = []\n",
    "        for skill in main_list.find_elements_by_class_name(\"pvs-entity\"):\n",
    "            self.skills.append(skill.find_element_by_tag_name(\"span\").find_element_by_tag_name(\"span\").text)\n",
    "            \n",
    "def get_courses(self):\n",
    "        url = os.path.join(self.linkedin_url, \"details/courses\")\n",
    "        self.driver.get(url)\n",
    "        self.focus()\n",
    "        main = self.wait_for_element_to_load(by=By.ID, name=\"main\")\n",
    "        self.scroll_to_half()\n",
    "        self.scroll_to_bottom()\n",
    "        main_list = self.wait_for_element_to_load(name=\"pvs-list\", base=main)\n",
    "        self.courses = []\n",
    "        for course in main_list.find_elements_by_class_name(\"pvs-entity\"):\n",
    "            self.courses.append(course.find_element_by_tag_name(\"span\").find_element_by_tag_name(\"span\").text)\n",
    "            \n",
    "def __repr__(self):\n",
    "        return \"{name}\\n\\nAbout\\n{about}\\n\\nLocation\\n{loc}\\n\\nExperience\\n{exp}\\n\\nEducation\\n{edu}\\n\\nSkills\\n{skills}\\n\\nCourses\\n{courses}\".format(\n",
    "            name=self.name,\n",
    "            about=self.about,\n",
    "            loc=self.location,\n",
    "            exp=self.experiences,\n",
    "            edu=self.educations,\n",
    "            skills=self.skills,\n",
    "            courses=self.courses,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7922d0fb",
   "metadata": {
    "id": "7922d0fb"
   },
   "outputs": [],
   "source": [
    "profile_urls = [] \n",
    "set_profile = []\n",
    "profiles = [['Software Developer',0], ['Business Analyst',0], ['Marketing Analyst',0], ['Sales Analyst',0], ['UX Designer',0], ['Product Manager',0], ['Account Manager',0], ['Machine Learning Engineer',0]]\n",
    "  \n",
    "\n",
    "def get_urls(profile_urls, set_profile, profiles):\n",
    "    data_points = len(profile_urls)\n",
    "    while data_points <= 1000:\n",
    "        job = random.randrange(len(profiles))\n",
    "        ctr = profiles[job][1]\n",
    "\n",
    "        query = 'https://google.com/search?q=site:linkedin.com/in AND ' + profiles[job][0] +'&start='+str(ctr)\n",
    "        response = requests.get(query)\n",
    "\n",
    "        delays = [7, 6, 10, 19]\n",
    "        delay = np.random.choice(delays)\n",
    "        time.sleep(delay)\n",
    "\n",
    "        soup = BeautifulSoup(response.text,'html.parser')\n",
    "        for anchor in soup.find_all('a'):\n",
    "            url = anchor[\"href\"]\n",
    "\n",
    "            if '//support.google.com/websearch/answer/86640' in url:\n",
    "                file = open(\"linkedIn_urls.pickle\",\"wb\")\n",
    "                pickle.dump(profile_urls, file)\n",
    "                file.close()\n",
    "\n",
    "                file = open(\"profile_data.pickle\",\"wb\")\n",
    "                pickle.dump(profiles, file)\n",
    "                file.close()\n",
    "\n",
    "                file = open(\"profile_set.pickle\",\"wb\")\n",
    "                pickle.dump(set_profile, file)\n",
    "                file.close()\n",
    "\n",
    "                return profile_urls\n",
    "\n",
    "            if 'https://www.linkedin.com/' in url:\n",
    "                url = url[7:url.find('&')]\n",
    "                if url not in set_profile:\n",
    "                    profile_urls.append([url, profiles[job][0]])\n",
    "                    set_profile.append(url)\n",
    "                    data_points = data_points + 1\n",
    "                    print(url)\n",
    "        profiles[job][1] = ctr+10\n",
    "    return profile_urls\n",
    "\n",
    "linkedIn_urls = get_urls(profile_urls, set_profile, profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "def8de2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def8de2d",
    "outputId": "5a2e20a3-b96b-4626-d8f8-cf2237efede0"
   },
   "outputs": [],
   "source": [
    "file = open(\"linkedIn_urls.pickle\",\"rb\")\n",
    "profile_urls = pickle.load(file) \n",
    "file.close()\n",
    "\n",
    "file = open(\"profile_set.pickle\",\"rb\")\n",
    "set_profile = pickle.load(file) \n",
    "file.close()\n",
    "\n",
    "file = open(\"profile_data.pickle\",\"rb\")\n",
    "profiles = pickle.load(file) \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "455b6cc8",
   "metadata": {
    "id": "455b6cc8",
    "outputId": "e2a3b11a-25a0-4790-dd16-9eeea1825c5d"
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "email = \"9971774111\"\n",
    "password = \"balaji123\"\n",
    "actions.login(driver, email, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a307ca2",
   "metadata": {
    "id": "6a307ca2",
    "outputId": "68734f88-7877-4394-d638-4a54c29d35ad"
   },
   "outputs": [],
   "source": [
    "file = open(\"linkedIn_dataset.pickle\",\"rb\")\n",
    "data_matrix = pickle.load(file) \n",
    "file.close()\n",
    "\n",
    "counter = len(data_matrix)\n",
    "url_profile_list = profile_urls[counter:]\n",
    "\n",
    "for point in url_profile_list:\n",
    "    url = point[0]\n",
    "    profile = point[1]\n",
    "    person1 = Person(url, driver=driver, scrape = False)\n",
    "    if scrape(person1, close_on_complete=False) != -1:\n",
    "        row = [__repr__(person1),[profile]]\n",
    "        data_matrix.append(row)\n",
    "        print(profile)\n",
    "    else:\n",
    "        row = [url,[profile]]\n",
    "        data_matrix.append(row)\n",
    "#         driver.quit()\n",
    "#         driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "#         email = \"rishita.chauhan2911@gmail.com\"\n",
    "#         password = \"348110076\"\n",
    "#         actions.login(driver, email, password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "faf1a3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"linkedIn_dataset.pickle\",\"wb\")\n",
    "pickle.dump(data_matrix, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "af1d3f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinkedIn Resume</th>\n",
       "      <th>Job Profiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shyvee Shi\\n(She/Her)\\n3rd degree connection\\n...</td>\n",
       "      <td>[Product Manager]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elisa Bellagamba\\n\\nAbout\\nHigh-impact product...</td>\n",
       "      <td>[Product Manager]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joni (Rafalski) Hoadley (She/Her)\\n\\nAbout\\nI’...</td>\n",
       "      <td>[Product Manager]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shane Connelly\\n\\nAbout\\nI lead the product ma...</td>\n",
       "      <td>[Product Manager]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dana Tom\\n(She/Her)\\n\\nAbout\\nI’m a product ma...</td>\n",
       "      <td>[Product Manager]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Johanna P.\\n\\nAbout\\nExperienced Bilingual Rep...</td>\n",
       "      <td>[Sales Analyst]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>Rachel Sedlacek\\n\\nAbout\\nAnalyst with 12 year...</td>\n",
       "      <td>[Sales Analyst]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Deirdre Derby\\n\\nAbout\\n[]\\n\\nLocation\\nMinnea...</td>\n",
       "      <td>[Sales Analyst]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Ryan Mitchell\\n\\nAbout\\nSr. Sales Analyst for ...</td>\n",
       "      <td>[Sales Analyst]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>Sara Cope\\n\\nAbout\\nExperienced Analyst with a...</td>\n",
       "      <td>[Sales Analyst]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>372 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       LinkedIn Resume       Job Profiles\n",
       "0    Shyvee Shi\\n(She/Her)\\n3rd degree connection\\n...  [Product Manager]\n",
       "1    Elisa Bellagamba\\n\\nAbout\\nHigh-impact product...  [Product Manager]\n",
       "2    Joni (Rafalski) Hoadley (She/Her)\\n\\nAbout\\nI’...  [Product Manager]\n",
       "3    Shane Connelly\\n\\nAbout\\nI lead the product ma...  [Product Manager]\n",
       "4    Dana Tom\\n(She/Her)\\n\\nAbout\\nI’m a product ma...  [Product Manager]\n",
       "..                                                 ...                ...\n",
       "407  Johanna P.\\n\\nAbout\\nExperienced Bilingual Rep...    [Sales Analyst]\n",
       "408  Rachel Sedlacek\\n\\nAbout\\nAnalyst with 12 year...    [Sales Analyst]\n",
       "409  Deirdre Derby\\n\\nAbout\\n[]\\n\\nLocation\\nMinnea...    [Sales Analyst]\n",
       "410  Ryan Mitchell\\n\\nAbout\\nSr. Sales Analyst for ...    [Sales Analyst]\n",
       "411  Sara Cope\\n\\nAbout\\nExperienced Analyst with a...    [Sales Analyst]\n",
       "\n",
       "[372 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(data = data_matrix, columns=['LinkedIn Resume', 'Job Profiles'])\n",
    "remove = data[data['LinkedIn Resume'].str.startswith('https://')].index\n",
    "data.drop(remove, axis = 0, inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8c1ba13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinkedIn Resume</th>\n",
       "      <th>Job Profiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [LinkedIn Resume, Job Profiles]\n",
       "Index: []"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['LinkedIn Resume'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "906e3c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sales Analyst                68\n",
       "Machine Learning Engineer    68\n",
       "Product Manager              62\n",
       "Marketing Analyst            51\n",
       "UX Designer                  47\n",
       "Software Developer           28\n",
       "Business Analyst             24\n",
       "Account Manager              24\n",
       "Name: Job Profiles, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = data['Job Profiles'].str[0]\n",
    "r.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2b0ff51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('LinkedIn_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
